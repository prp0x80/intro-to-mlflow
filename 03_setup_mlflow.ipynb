{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: MLflow Set Up\n",
    "execute:\n",
    "  echo: true\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will install MLflow and try out different configuration scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "The easiest way to install MLflow is using `pip` as follows -\n",
    "\n",
    ":::{.callout-note}\n",
    "Make sure you have created and activated the virtual environment using the python virtual environment manager of your choice.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --quiet mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow comes with a rich CLI that provides a simple interface to various functionality in MLflow. You can use the CLI to run projects, start the tracking UI, create and list experiments, download run artifacts, serve MLflow Python Function and scikit-learn models, and serve models on Microsoft Azure Machine Learning and Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!mlflow --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will use `mlflow server <args>` and `mlflow ui` commands to demonstrate different MLflow set up scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common MLflow configurations\n",
    "\n",
    "Since the MLflow client can interface with a variety of backend and artifact storage configurations. We will look a three common scenarios:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario#1\n",
    "\n",
    "> MLflow on localhost  \n",
    "\n",
    "This is the most basic set up, where both backend and artifact store are set to local file store. It's the default mode, so we don't have to set any parameters while starting the tracking server.  \n",
    "\n",
    "Start MLflow tracking server without any arguments as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```bash\n",
    "mlflow server\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/scenario1.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario#2\n",
    "\n",
    "> MLflow on localhost with backend store as an SQLAlchemy compatible database type: SQLite\n",
    "\n",
    "In this case, artifacts are stored under a local directory, and MLflow entities are inserted in a SQLite database file `mlruns.db`.\n",
    "\n",
    "Start MLflow tracking server by specifying appropriate values for `--backend-store-uri` and `--default-artifact-root` as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```bash\n",
    "mlflow server --backend-store-uri sqlite:////workspace/mlruns.db \\\n",
    "              --default-artifact-root /workspace/mlruns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/scenario2.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario#3\n",
    "\n",
    "> Tracking server launched at localhost\n",
    "\n",
    "Similar to scenario 1 but a tracking server is launched, listening for REST request calls at the default port 5000.\n",
    "\n",
    "Start MLflow tracking server by specifying local file path value for `--backend-store-uri` as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```bash\n",
    "mlflow server --backend-store-uri /workspace/mlruns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/scenario3.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Tracking UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tracking server is up using the scenario of choice, you can launch MLflow tracking UI by typing `mlflow ui --port 6000` in a separate terminal.  \n",
    "\n",
    "At this point you should have two MLflow services running locally:\n",
    "\n",
    "- MLflow Tracking server running at `http://localhost:5000`\n",
    "- MLflow Tracking UI at `http://localhost:6000`\n",
    "\n",
    "MLFlow Tracking UI is in constant communication with the tracking server to present the results of each experiment from the set storage location. It also, allows to compare results across runs and experiments.\n",
    "\n",
    "![](./images/mlflow_ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Oct  4 2022, 14:00:32) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ac03a0a6051494cc606d484d27d20fce22fb7b4d169f583271e11d5ba46a56e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
