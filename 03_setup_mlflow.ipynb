{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "  echo: true\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up MLflow\n",
    "\n",
    "In this section, we will apply the MLflow concepts we have learned so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "The easiest way to install MLflow is using `pip` as follows -\n",
    "\n",
    ":::{.callout-note}\n",
    "Make sure you have created and activated the virtual environment using the python virtual environment manager of your choice.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --quiet mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow comes with a rich CLI that provides a simple interface to various functionality in MLflow. You can use the CLI to run projects, start the tracking UI, create and list experiments, download run artifacts, serve MLflow Python Function and scikit-learn models, and serve models on Microsoft Azure Machine Learning and Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!mlflow --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will use `mlflow server <args>` and `mlflow ui` commands to demonstrate different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario#1\n",
    "\n",
    "> MLflow on localhost  \n",
    "\n",
    "This is the most basic set up, where both backend and artifact store are set to local file store. It's the default mode, so we don't have to set any parameters while starting the tracking server.  \n",
    "\n",
    "Start MLflow tracking server without any arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#| output: false\n",
    "mlflow server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario#2\n",
    "\n",
    "> MLflow on localhost with backend store as an SQLAlchemy compatible database type: SQLite\n",
    "\n",
    "In this case, artifacts are stored under a local directory, and MLflow entities are inserted in a SQLite database file `mlruns.db`.\n",
    "\n",
    "Start MLflow tracking server by specifying appropriate values for `--backend-store-uri` and `--default-artifact-root`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#| output: false\n",
    "mlflow server --backend-store-uri sqlite:////workspace/mlruns.db \\\n",
    "              --default-artifact-root /workspace/mlruns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario#3\n",
    "\n",
    "> Tracking server launched at localhost\n",
    "\n",
    "Similar to scenario 1 but a tracking server is launched, listening for REST request calls at the default port 5000.\n",
    "\n",
    "Start MLflow tracking server by specifying local file path value for `--backend-store-uri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#| output: false\n",
    "mlflow server --backend-store-uri /workspace/mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Tracking UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the scenario is set up, you can launch MLflow tracking UI by typing `mlflow ui --port 6000` in a separate terminal.  \n",
    "\n",
    "\n",
    "At this point you would have MLflow tracking server running on port 5000 (`http://localhost:5000`) and MLflow Tracking UI running on port 6000. MLFlow Tracking UI communicates with the server to present results of each experiment from the set storage location. It also, allows to compare results across runs and experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ac03a0a6051494cc606d484d27d20fce22fb7b4d169f583271e11d5ba46a56e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
