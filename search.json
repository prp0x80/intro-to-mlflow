[
  {
    "objectID": "01_ml_complexities.html#traditional-software-vs.-machine-learning",
    "href": "01_ml_complexities.html#traditional-software-vs.-machine-learning",
    "title": "Data & AI Engineering Academy",
    "section": "Traditional Software vs. Machine Learning",
    "text": "Developing machine learning applications is complex, and the complexity doesn’t necessarity strive from the fact that the theory behind the machine learning is difficult, or the math is difficult or the algorithm which are presented are difficult. Although there is some element of complexity, the machine learning frameworks makes it easier by abstracting away the details and exposing a simple class interface to initialise and train the model. In fact the default paramters gives pretty much a good baseline model to work with.\n\n\n\n\n\n\n\nTraditional Software\nMachine Learning\n\n\n\n\nGoal: Meet a functional specification\nGoal: Optimize metric (e.g. accuracy) Constantly experiment to improve it.\n\n\nQuality depends only on code\nQuality depends on input data and tuning parmeters\n\n\nTypically pick one software stack w/ fewer libraries and tools\nCompare + combine many libraries, models\n\n\nLimited deployment environments\nDiverse deployment environments"
  },
  {
    "objectID": "01_ml_complexities.html#machine-learning-lifecycle",
    "href": "01_ml_complexities.html#machine-learning-lifecycle",
    "title": "Data & AI Engineering Academy",
    "section": "Machine Learning Lifecycle",
    "text": "Following are four stages of a machine learning lifecycle, there can be other stages is between, but these are the paramount stages -\n\nData Ingestion - Depending on the nature of data you might use one of the many data technologies for staging the data e.g., Hadoop, Kafka, S3, Delta lake, etc.\nData Preparation - As per requirement and ease of use you might use a library in different language for preparing the data e.g., Spark, Pandas, Scikit-Learn, R, Java, etc.\nTraining - Depending on the problem and type of data you might use different libraries or framework for training the model e.g., Scikit-Learn, TensorFlow, PyTorch, Xgboost, etc.\nDeployment - Based on the type of model and how you want to serve the results there are many deployment options like Docker, Kubernetes, TensorFlow serving, Flask, etc."
  },
  {
    "objectID": "01_ml_complexities.html#challenges-in-managing-machine-learning-lifecycle",
    "href": "01_ml_complexities.html#challenges-in-managing-machine-learning-lifecycle",
    "title": "Data & AI Engineering Academy",
    "section": "Challenges in managing Machine Learning lifecycle",
    "text": "Each stage has it’s own requirements and tools\nSome stages (data preparation and training) rely on tuning parameters\nEach stage has it’s own scaling requirements\nEnsuring the same model that yielded optimal performance is deployed\nGovernance and provenance - how the model evolved, who used it, when it was used so on and so forth.\n\nTo solve these challenges, MLflow, an open source project, simplifies the entire ML lifecycle. MLflow introduces simple abstractions to package reproducible projects, track results, encapsulate models that can be used with many existing tools, and central repository to share models, accelerating the ML lifecycle for organizations of any size."
  },
  {
    "objectID": "02_mlflow_intro.html#mlflow-components",
    "href": "02_mlflow_intro.html#mlflow-components",
    "title": "Data & AI Engineering Academy",
    "section": "MLflow Components",
    "text": "MLflow currently offers four components:\n\n\n\n\n\n\n\nNote\n\n\n\nMLflow Models and MLflow Registry are not within the scope of this training.\n\n\n\nMLflow Tracking\nWhen you use MLflow model tracking, you can train a variety of different machine learning models then make predictions with them interchangeably using the standardized model prediction interface. You can also register your models in the MLflow model registry and keep track of which model is being used in production so that this information is easily accessible to everyone you are working with.\nMLflow Tracking is organized around the concept of runs, which are executions of some piece of data science code. Each run records the following information -\n\nParameters: Key-value inputs to your code\nMetrics: Numeric values (can update over time)\nTags and Notes: Additional information about a run\nArtifacts: Files, data, and models\nSource: Name of the file used to launch the run\nVersion: The version of the source code\nRun: An instance of code that runs by MLflow\nExperiment: {Run, …, Run}\nStart & End Time: Start and end time of a run\n\n\n\n\nRuns and Artifacts Store\nMLFlow provides wide variety of storage option for logging runs and artifacts.\n\nMLflow Runs\n\nThey can be recorded to local files, to a SQLAlchemy compatible database, or remotely to a tracking server.\nMLflow uses backend store component for storing runs\nBackend store persists MLflow entities (runs, parameters, metrics, tags, notes, metadata, etc.)\nBackend store options:\n\nA file store backend - local file path\nA database-backed store - mysql, mssql, sqlite, or postgresql\nHTTP server (specified as https://my-server:5000), which is a server hosting an MLflow tracking server.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, the MLflow Python API logs runs locally to files in an mlruns directory wherever you ran your program. You can then run mlflow ui to see the logged runs.\n\n\n\n\nMLflow Artifacts\n\nThey can be persisted to local files and a variety of remote file storage solutions.\nMLflow uses artifact store component for storing artifacts\nArtifact store persists artifacts (files, models, images, in-memory objects, or model summary, etc.)\nArtifact store options:\n\nLocal file path\nAmazon S3\nAzure Blob Storage\nGoogle Cloud Storage\nSFTP Server\nNFS\n\n\n\n\n\nMLflow Models"
  },
  {
    "objectID": "03_setup_mlflow.html#install",
    "href": "03_setup_mlflow.html#install",
    "title": "Data & AI Engineering Academy",
    "section": "Install",
    "text": "The easiest way to install MLflow is using pip as follows -\n\n\n\n\n\n\nNote\n\n\n\nMake sure you have created and activated the virtual environment using the python virtual environment manager of your choice.\n\n\n!pip install --upgrade pip\n!pip install --quiet mlflow\nMLflow comes with a rich CLI that provides a simple interface to various functionality in MLflow. You can use the CLI to run projects, start the tracking UI, create and list experiments, download run artifacts, serve MLflow Python Function and scikit-learn models, and serve models on Microsoft Azure Machine Learning and Amazon SageMaker.\n!mlflow --help\nIn the next section, we will use mlflow server <args> and mlflow ui commands to demonstrate different MLflow set up scenarios."
  },
  {
    "objectID": "03_setup_mlflow.html#common-mlflow-configurations",
    "href": "03_setup_mlflow.html#common-mlflow-configurations",
    "title": "Data & AI Engineering Academy",
    "section": "Common MLflow configurations",
    "text": "Since the MLflow client can interface with a variety of backend and artifact storage configurations. We will look a three common scenarios:\n\nScenario#1\n\nMLflow on localhost\n\nThis is the most basic set up, where both backend and artifact store are set to local file store. It’s the default mode, so we don’t have to set any parameters while starting the tracking server.\nStart MLflow tracking server without any arguments as follows:\nmlflow server\n\n\n\nScenario#2\n\nMLflow on localhost with backend store as an SQLAlchemy compatible database type: SQLite\n\nIn this case, artifacts are stored under a local directory, and MLflow entities are inserted in a SQLite database file mlruns.db.\nStart MLflow tracking server by specifying appropriate values for --backend-store-uri and --default-artifact-root as follows:\nmlflow server --backend-store-uri sqlite:////workspace/mlruns.db \\\n              --default-artifact-root /workspace/mlruns\n\n\n\nScenario#3\n\nTracking server launched at localhost\n\nSimilar to scenario 1 but a tracking server is launched, listening for REST request calls at the default port 5000.\nStart MLflow tracking server by specifying local file path value for --backend-store-uri as follows:\nmlflow server --backend-store-uri /workspace/mlruns"
  },
  {
    "objectID": "03_setup_mlflow.html#mlflow-tracking-ui",
    "href": "03_setup_mlflow.html#mlflow-tracking-ui",
    "title": "Data & AI Engineering Academy",
    "section": "MLflow Tracking UI",
    "text": "After the tracking server is up using the scenario of choice, you can launch MLflow tracking UI by typing mlflow ui --port 6000 in a separate terminal.\nAt this point you should have two MLflow services running locally:\n\nMLflow Tracking server running at http://localhost:5000\nMLflow Tracking UI at http://localhost:6000\n\nMLFlow Tracking UI is in constant communication with the tracking server to present the results of each experiment from the set storage location. It also, allows to compare results across runs and experiments."
  },
  {
    "objectID": "04_mlflow_example.html#training-employee-attrition-detection-model",
    "href": "04_mlflow_example.html#training-employee-attrition-detection-model",
    "title": "Data & AI Engineering Academy",
    "section": "Training employee attrition detection model",
    "text": "Objective Predict if an employee is likely to quit and identify the factors responsible - to allow HR to intervene on time and remedy the situation to prevent attrition.\n\nWhile some level of attrition in a company is inevitable, minimizing it and being prepared for the cases that cannot be helped will significantly help improve the operations of most businesses.\n\nData The data set presents an employee survey from IBM, indicating if there is attrition or not. The data set contains approximately 1500 entries. Given the limited size of the data set, the model should only be expected to provide modest improvement in indentification of attrition vs a random allocation of probability of attrition.\n\nImport libraries\nfrom typing import Tuple\n\nimport matplotlib.pyplot as plt\nimport mlflow\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    f1_score,\n    plot_confusion_matrix,\n    precision_score,\n    recall_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n\nLoad Data\ndef load_csv_data(path: str) -> pd.DataFrame:\n    \"\"\"Loads the csv file from path and returns pandas dataframe\"\"\"\n    try:\n        data = pd.read_csv(path)\n        return data\n    except:\n        raise Exception(f\"Error while loading the data from {path}\")\n\n\nTrain-test split\ndef split_data(\n    df: pd.DataFrame, test_size: float = 0.2, random_state: int = 42\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Splits the input data and returns training and test sets\"\"\"\n    drop_columns = [\n        \"EmployeeNumber\",\n        \"EmployeeCount\",\n        \"Over18\",\n        \"StandardHours\",\n        \"Attrition\",\n    ]\n    label_column = \"Attrition\"\n    features = df.drop(columns=drop_columns)\n    labels = df[label_column]\n    train_features, test_features, train_labels, test_labels = train_test_split(\n        features,\n        labels,\n        test_size=test_size,\n        random_state=random_state,\n        stratify=labels,\n    )\n    return train_features, test_features, train_labels, test_labels\n\n\nTraining\nUsing Multilayer perceptron algorithm\ndef train(\n    train_features: pd.DataFrame,\n    train_labels: pd.DataFrame,\n    random_state: int = 42,\n    **kwargs,\n) -> Tuple[LabelEncoder, Pipeline]:\n    \"\"\"Preprocesses the data and trains a model using sklearn pipeline\"\"\"\n    # label encoder\n    label_encoder = LabelEncoder()\n    label_encoder.fit(train_labels.values)\n    train_labels = label_encoder.transform(train_labels)\n    # pipeline\n    categorical_columns = [\n        \"BusinessTravel\",\n        \"Department\",\n        \"Education\",\n        \"EducationField\",\n        \"EnvironmentSatisfaction\",\n        \"Gender\",\n        \"JobInvolvement\",\n        \"JobLevel\",\n        \"JobRole\",\n        \"JobSatisfaction\",\n        \"MaritalStatus\",\n        \"OverTime\",\n        \"PerformanceRating\",\n        \"RelationshipSatisfaction\",\n        \"StockOptionLevel\",\n        \"WorkLifeBalance\",\n    ]\n    numerical_columns = train_features.select_dtypes(include=\"int64\").columns\n    transformers = [\n        (\"one_hot_encoder\", OneHotEncoder(), categorical_columns),\n        (\"scaler\", StandardScaler(), numerical_columns),\n    ]\n    preprocessing = ColumnTransformer(transformers=transformers)\n    classifier = MLPClassifier(\n        max_iter=kwargs.get(\"max_iter\", 500),\n        activation=kwargs.get(\"activation\", \"tanh\"),\n        solver=kwargs.get(\"solver\", \"sgd\"),\n        random_state=random_state,\n    )\n    model = Pipeline(\n        steps=[(\"preprocessing\", preprocessing), (\"classifier\", classifier)]\n    )\n    # model building - fit\n    model.fit(train_features, train_labels)\n    return label_encoder, model\n\n\nMetrics\nAccuracy * Proportion of true results among the total number of cases * Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or No class imbalance\nPrecision * Proportion of predicted positives are truly positives * Precision is a valid choice of evaluation metric when we want to be very sure of our prediction\nRecall * What proportion of actual Positives is correctly classified? * Recall is a valid choice of evaluation metric when we want to capture as many positives as possible * For example: If we are building a system to predict if a person has cancer or not, we want to capture the disease even if we are not very sure.\nF1-score * The F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall * We use this when we want to have a model with both good precision and recall * If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.\ndef evaluate(\n    model: Pipeline,\n    label_encoder: LabelEncoder,\n    test_features: pd.DataFrame,\n    test_labels: pd.DataFrame,\n) -> Tuple[float, float, float, float]:\n    \"\"\"Evaluates the trained model using the held out test set\"\"\"\n    predictions = model.predict(test_features)\n    test_labels = label_encoder.transform(test_labels)\n    accuracy = accuracy_score(test_labels, predictions)\n    precision = precision_score(test_labels, predictions)\n    recall = recall_score(test_labels, predictions)\n    f1 = f1_score(test_labels, predictions)\n    cm = confusion_matrix(test_labels, predictions)\n    plot_confusion_matrix(\n        model, test_features, test_labels, display_labels=label_encoder.classes_\n    )\n    return accuracy, precision, recall, f1, cm, plt\n\n\nExperiments\ndef run_experiment(experiment_name, **parameters):\n    \"\"\"Runs all the steps and logs experiment parameters using mlflow\"\"\"\n    with mlflow.start_run(run_name=experiment_name) as run:\n        run_id = run.info.run_uuid\n        experiment_id = run.info.experiment_id\n        print(f\"\\nRun Id\", run_id)\n        print(f\"Experiment Id\", experiment_id)\n        data = load_csv_data(\"https://tinyurl.com/ibmhrattrition\")\n        train_features, test_features, train_labels, test_labels = split_data(data)\n        label_encoder, model = train(train_features, train_labels, **parameters)\n        mlflow.sklearn.log_model(model, \"hr-attrition-model\")\n        mlflow.log_params(parameters)\n        accuracy, precision, recall, f1, cm, plt_cm = evaluate(\n            model, label_encoder, test_features, test_labels\n        )\n        tp = cm[0][0]\n        tn = cm[1][1]\n        fp = cm[0][1]\n        fn = cm[1][0]\n        mlflow.log_metric(\"TP\", tp)\n        mlflow.log_metric(\"TN\", tn)\n        mlflow.log_metric(\"FP\", fp)\n        mlflow.log_metric(\"FN\", fn)\n        print(\"accuracy_score\", accuracy)\n        print(\"precision_score\", precision)\n        print(\"recall_score\", recall)\n        print(\"f1_score\", f1)\n        mlflow.log_metric(\"accuracy_score\", accuracy)\n        mlflow.log_metric(\"precision_score\", precision)\n        mlflow.log_metric(\"recall_score\", recall)\n        mlflow.log_metric(\"f1_score\", f1)\n        fig_name = \"confusion-matrix.png\"\n        plt_cm.savefig(fig_name)\n        mlflow.log_artifact(fig_name, \"confusion-matrix-plot\")\n        return run_id, experiment_id\n# change parameters and run experiments\nrun_experiment(\n    \"hr-attrition-experiment\", max_iter=500, activation=\"relu\", solver=\"adam\"\n)\n# run_experiment(\"hr-attrition-experiment\", max_iter=1000, activation=\"relu\", solver=\"adam\")\n# run_experiment(\"hr-attrition-experiment\", max_iter=500, activation=\"relu\", solver=\"sgd\")\n# run_experiment(\"hr-attrition-experiment\", max_iter=1000, activation=\"relu\", solver=\"sgd\")\n# run_experiment(\"hr-attrition-experiment\", max_iter=500, activation=\"tanh\", solver=\"sgd\")\n# run_experiment(\"hr-attrition-experiment\", max_iter=500, activation=\"tanh\", solver=\"adam\")\n\n\n\nPredict\ndata = load_csv_data(\"https://tinyurl.com/ibmhrattrition\")\ntrain_features, test_features, train_labels, test_labels = split_data(data)\ndata.head()\ndef predict(features):\n    run_id = \"72a2690bd0c24316984f4e2f9e49f3bd\"\n    logged_model = f\"file:///content/mlruns/0/{run_id}/artifacts/hr-attrition-model\"\n    loaded_model = mlflow.pyfunc.load_model(logged_model)\n    response = loaded_model.predict(pd.DataFrame(features))\n    return \"No\" if response[0] == 0 else \"Yes\"\ntest_features\nsample_features = test_features.loc[[1023]]\nsample_features\npredict(sample_features)\ntest_labels.loc[[1023]]"
  }
]