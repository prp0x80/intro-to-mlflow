[
  {
    "objectID": "01_ml_complexities.html",
    "href": "01_ml_complexities.html",
    "title": "ML Challenges",
    "section": "",
    "text": "Machine Learning (ML) development brings many new complexities beyond the traditional software development lifecycle. Unlike in traditional software development, ML developers want to try multiple algorithms, tools and parameters to get the best results, and they need to track this information to reproduce work. In addition, developers need to use many distinct systems to productionize models."
  },
  {
    "objectID": "01_ml_complexities.html#traditional-software-vs.-machine-learning",
    "href": "01_ml_complexities.html#traditional-software-vs.-machine-learning",
    "title": "ML Challenges",
    "section": "Traditional Software vs. Machine Learning",
    "text": "Traditional Software vs. Machine Learning\nDeveloping machine learning applications is complex, and the complexity doesn’t necessarity strive from the fact that the theory behind the machine learning is difficult, or the math is difficult or the algorithm which are presented are difficult. Although there is some element of complexity, the machine learning frameworks makes it easier by abstracting away the details and exposing a simple class interface to initialise and train the model. In fact the default paramters gives pretty much a good baseline model to work with.\n\n\n\n\n\n\n\nTraditional Software\nMachine Learning\n\n\n\n\nGoal: Meet a functional specification\nGoal: Optimize metric (e.g. accuracy) Constantly experiment to improve it.\n\n\nQuality depends only on code\nQuality depends on input data and tuning parmeters\n\n\nTypically pick one software stack w/ fewer libraries and tools\nCompare + combine many libraries, models\n\n\nLimited deployment environments\nDiverse deployment environments"
  },
  {
    "objectID": "01_ml_complexities.html#machine-learning-lifecycle",
    "href": "01_ml_complexities.html#machine-learning-lifecycle",
    "title": "ML Challenges",
    "section": "Machine Learning Lifecycle",
    "text": "Machine Learning Lifecycle\nFollowing are four stages of a machine learning lifecycle, there can be other stages is between, but these are the paramount stages -\n\nData Ingestion - Depending on the nature of data you might use one of the many data technologies for staging the data e.g., Hadoop, Kafka, S3, Delta lake, etc.\nData Preparation - As per requirement and ease of use you might use a library in different language for preparing the data e.g., Spark, Pandas, Scikit-Learn, R, Java, etc.\nTraining - Depending on the problem and type of data you might use different libraries or framework for training the model e.g., Scikit-Learn, TensorFlow, PyTorch, Xgboost, etc.\nDeployment - Based on the type of model and how you want to serve the results there are many deployment options like Docker, Kubernetes, TensorFlow serving, Flask, etc."
  },
  {
    "objectID": "01_ml_complexities.html#challenges-in-managing-machine-learning-lifecycle",
    "href": "01_ml_complexities.html#challenges-in-managing-machine-learning-lifecycle",
    "title": "ML Challenges",
    "section": "Challenges in managing Machine Learning lifecycle",
    "text": "Challenges in managing Machine Learning lifecycle\n\nEach stage has it’s own requirements and tools\nSome stages (data preparation and training) rely on tuning parameters\nEach stage has it’s own scaling requirements\nEnsuring the same model that yielded optimal performance is deployed\nGovernance and provenance - how the model evolved, who used it, when it was used so on and so forth.\n\nTo solve these challenges, MLflow, an open source project, simplifies the entire ML lifecycle. MLflow introduces simple abstractions to package reproducible projects, track results, encapsulate models that can be used with many existing tools, and central repository to share models, accelerating the ML lifecycle for organizations of any size."
  },
  {
    "objectID": "02_mlflow_intro.html",
    "href": "02_mlflow_intro.html",
    "title": "Introducing MLflow",
    "section": "",
    "text": "MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry."
  },
  {
    "objectID": "02_mlflow_intro.html#mlflow-components",
    "href": "02_mlflow_intro.html#mlflow-components",
    "title": "Introducing MLflow",
    "section": "MLflow Components",
    "text": "MLflow Components\nMLflow currently offers four components:\n\n\n\n\n\n\n\nNote\n\n\n\nMLflow Models and MLflow Registry are not within the scope of this training.\n\n\n\nMLflow Tracking\nWhen you use MLflow model tracking, you can train a variety of different machine learning models then make predictions with them interchangeably using the standardized model prediction interface. You can also register your models in the MLflow model registry and keep track of which model is being used in production so that this information is easily accessible to everyone you are working with.\nMLflow Tracking is organized around the concept of runs, which are executions of some piece of data science code. Each run records the following information -\n\nParameters: Key-value inputs to your code\nMetrics: Numeric values (can update over time)\nTags and Notes: Additional information about a run\nArtifacts: Files, data, and models\nSource: Name of the file used to launch the run\nVersion: The version of the source code\nRun: An instance of code that runs by MLflow\nExperiment: {Run, …, Run}\nStart & End Time: Start and end time of a run\n\n\n\n\nRuns and Artifacts Store\nMLFlow provides wide variety of storage option for logging runs and artifacts.\n\nMLflow Runs\n\nThey can be recorded to local files, to a SQLAlchemy compatible database, or remotely to a tracking server.\nMLflow uses backend store component for storing runs\nBackend store persists MLflow entities (runs, parameters, metrics, tags, notes, metadata, etc.)\nBackend store options:\n\nA file store backend - local file path\nA database-backed store - mysql, mssql, sqlite, or postgresql\nHTTP server (specified as https://my-server:5000), which is a server hosting an MLflow tracking server.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, the MLflow Python API logs runs locally to files in an mlruns directory wherever you ran your program. You can then run mlflow ui to see the logged runs.\n\n\n\n\nMLflow Artifacts\n\nThey can be persisted to local files and a variety of remote file storage solutions.\nMLflow uses artifact store component for storing artifacts\nArtifact store persists artifacts (files, models, images, in-memory objects, or model summary, etc.)\nArtifact store options:\n\nLocal file path\nAmazon S3\nAzure Blob Storage\nGoogle Cloud Storage\nSFTP Server\nNFS\n\n\n\n\n\nMLflow Models"
  },
  {
    "objectID": "03_mlflow_setup.html",
    "href": "03_mlflow_setup.html",
    "title": "MLflow Set Up",
    "section": "",
    "text": "In this section, we will install MLflow and try out different configuration scenarios."
  },
  {
    "objectID": "03_mlflow_setup.html#install",
    "href": "03_mlflow_setup.html#install",
    "title": "MLflow Set Up",
    "section": "Install",
    "text": "Install\nThe easiest way to install MLflow is using pip as follows -\n\n\n\n\n\n\nNote\n\n\n\nMake sure you have created and activated the virtual environment using the python virtual environment manager of your choice.\n\n\npip install --upgrade pip\npip install --quiet mlflow\nMLflow comes with a rich CLI that provides a simple interface to various functionality in MLflow. You can use the CLI to run projects, start the tracking UI, create and list experiments, download run artifacts, serve MLflow Python Function and scikit-learn models, and serve models on Microsoft Azure Machine Learning and Amazon SageMaker.\n\n!mlflow --help\n\nUsage: mlflow [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --version  Show the version and exit.\n  --help     Show this message and exit.\n\nCommands:\n  artifacts    Upload, list, and download artifacts from an MLflow...\n  azureml      Serve models on Azure ML.\n  db           Commands for managing an MLflow tracking database.\n  deployments  Deploy MLflow models to custom targets.\n  experiments  Manage experiments.\n  gc           Permanently delete runs in the `deleted` lifecycle stage.\n  models       Deploy MLflow models locally.\n  pipelines    Run MLflow Pipelines and inspect pipeline results.\n  run          Run an MLflow project from the given URI.\n  runs         Manage runs.\n  sagemaker    Serve models on SageMaker.\n  server       Run the MLflow tracking server.\n  ui           Launch the MLflow tracking UI for local viewing of run...\n\n\nIn the next section, we will use mlflow server <args> and mlflow ui commands to demonstrate different MLflow set up scenarios."
  },
  {
    "objectID": "03_mlflow_setup.html#common-mlflow-configurations",
    "href": "03_mlflow_setup.html#common-mlflow-configurations",
    "title": "MLflow Set Up",
    "section": "Common MLflow configurations",
    "text": "Common MLflow configurations\nSince the MLflow client can interface with a variety of backend and artifact storage configurations. We will look a three common scenarios:\n\nScenario#1\n\nMLflow on localhost\n\nThis is the most basic set up, where both backend and artifact store are set to local file store. It’s the default mode, so we don’t have to set any parameters while starting the tracking server.\nStart MLflow tracking server without any arguments as follows:\nmlflow server\n\n\n\n\n\n\n\nScenario#2\n\nMLflow on localhost with backend store as an SQLAlchemy compatible database type: SQLite\n\nIn this case, artifacts are stored under a local directory, and MLflow entities are inserted in a SQLite database file mlruns.db.\nStart MLflow tracking server by specifying appropriate values for --backend-store-uri and --default-artifact-root as follows:\nmlflow server --backend-store-uri sqlite:////workspace/mlruns.db \\\n              --default-artifact-root /workspace/mlruns\n\n\n\n\n\n\n\nScenario#3\n\nTracking server launched at localhost\n\nSimilar to scenario 1 but a tracking server is launched, listening for REST request calls at the default port 5000.\nStart MLflow tracking server by specifying local file path value for --backend-store-uri as follows:\nmlflow server --backend-store-uri /workspace/mlruns\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo make the CLI commands aware of the tracking server set the MLFLOW_TRACKING_URI environment variable as follows:\nexport MLFLOW_TRACKING_URI=\"http://localhost:5000\""
  },
  {
    "objectID": "03_mlflow_setup.html#mlflow-tracking-ui",
    "href": "03_mlflow_setup.html#mlflow-tracking-ui",
    "title": "MLflow Set Up",
    "section": "MLflow Tracking UI",
    "text": "MLflow Tracking UI\nAfter the tracking server is up using the scenario of choice, you can launch MLflow tracking UI by typing http://localhost:5000 in the browser.\nMLFlow Tracking UI is in constant communication with the tracking server to present the results of each experiment from the set storage location. The UI contains the following key features:\n\nExperiment-based run listing and comparison (including run comparison across multiple experiments)\nSearching for runs by parameter or metric value\nVisualizing run metrics\nDownloading run results"
  },
  {
    "objectID": "04_mlflow_example.html",
    "href": "04_mlflow_example.html",
    "title": "MLflow in Practice",
    "section": "",
    "text": "In this section, we will apply the MLflow concepts we have learned so far."
  },
  {
    "objectID": "04_mlflow_example.html#employee-attrition-detection",
    "href": "04_mlflow_example.html#employee-attrition-detection",
    "title": "MLflow in Practice",
    "section": "Employee Attrition Detection",
    "text": "Employee Attrition Detection\nObjective\nPredict if an employee is likely to quit and identify the factors responsible - to allow HR to intervene on time and remedy the situation to prevent attrition. While some level of attrition in a company is inevitable, minimizing it and being prepared for the cases that cannot be helped will significantly help improve the operations of most businesses.\nData\nThe data set presents an employee survey from IBM, indicating if there is attrition or not. The data set contains approximately 1500 entries. Given the limited size of the data set, the model should only be expected to provide modest improvement in indentification of attrition vs a random allocation of probability of attrition.\n\nImport libraries\nMLflow supports wide variety of ML frameworks. For this example we will use the simple and popular sklearn library for training a model. Additionally, for clarity, we have abstracted the steps for ML classification into a kainos_sklearn package.\n\nimport warnings\n\nimport mlflow\nfrom mlflow.models.signature import infer_signature\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import (\n    MinMaxScaler,\n    OneHotEncoder,\n    OrdinalEncoder,\n    RobustScaler,\n    StandardScaler,\n)\n\nfrom kainos_sklearn.classifier import *\n\nwarnings.filterwarnings(action=\"ignore\")\n\n\n\nLoad data\nThe hr attrition data is available in csv format, so we can easily load it into a pandas dataframe.\n\ndata = load_csv(\"./data/hr_attrition.csv\")\n\nn_rows, n_cols = data.shape\nprint(f\"Rows: {n_rows}, Columns: {n_cols}\")\n\nRows: 1470, Columns: 35\n\n\n\n\nData Config\nThe outcome of machine learning algorithm also depends on the tuning we apply to the data, hence it’s important to keep a track of those too.\n\ndrop_cols = [\n    \"EmployeeNumber\",\n    \"EmployeeCount\",\n    \"Over18\",\n    \"StandardHours\",\n    \"Attrition\",\n]\n\nlabel_col = \"Attrition\"\n\ntest_size = 0.2\ncategory_threshold = 8\nfeature_encoder = OneHotEncoder\nfeature_scaler = StandardScaler\n\n\n\nML Config\nML parameters are another import set of values to be tracked. Each algorithm has it’s own parameters and our goal of our experiments is to find the parameters that works best for achieving our objective.\n\ntracking_uri = \"http://localhost:5000\"\nexperiment_name = \"attrition_rf\"\n\n# set sklearn classifer model and it's parameters\nsklearn_classifier = RandomForestClassifier\nmodel_params = dict(n_estimators=100, criterion=\"gini\", random_state=42)\n\nmlflow.set_tracking_uri(tracking_uri)\nmlflow.set_experiment(experiment_name);\n\n\n\nExperiments\nThis is an iterative step. Here we infuse the code with MLflow to allow us to record the data/ml parameters and metrics. The idea is to set the values in previous two cells (data and ml config) and run the experiment. The process is to be repeated until desired results are achieved.\n\nwith mlflow.start_run() as run:\n\n    # log data and ml config\n    mlflow.log_param(\"feature_encoder\", feature_encoder.__name__)\n    mlflow.log_param(\"feature_scaler\", feature_scaler.__name__)\n    mlflow.log_param(\"sklearn_classifier\", sklearn_classifier.__name__)\n    mlflow.log_param(\"test_size\", test_size)\n    mlflow.log_params(model_params)\n\n    # split data\n    train_X, train_y, test_X, test_y = split_data(\n        data, drop_cols=drop_cols, label_col=label_col, test_size=test_size\n    )\n\n    # train model\n    model, labels_encoder = train(\n        train_X,\n        train_y,\n        feature_encoder,\n        feature_scaler,\n        sklearn_classifier,\n        category_threshold,\n    )\n\n    # log model\n    signature = infer_signature(train_X, model.predict(train_X.sample(100)))\n    mlflow.sklearn.log_model(model, \"hr-attrition-model\", signature=signature)\n\n    # evaluate on test data using trained model\n    accuracy, precision, recall, f1, cm, plt_cm = evaluate(\n        model, labels_encoder, test_X, test_y\n    )\n\n    # log metrics\n    tp = cm[0][0]\n    tn = cm[1][1]\n    fp = cm[0][1]\n    fn = cm[1][0]\n    mlflow.log_metric(\"TP\", tp)\n    mlflow.log_metric(\"TN\", tn)\n    mlflow.log_metric(\"FP\", fp)\n    mlflow.log_metric(\"FN\", fn)\n    mlflow.log_metric(\"accuracy_score\", accuracy)\n    mlflow.log_metric(\"precision_score\", precision)\n    mlflow.log_metric(\"recall_score\", recall)\n    mlflow.log_metric(\"f1_score\", f1)\n\n    # log figure as artifact\n    fig_name = \"confusion-matrix.png\"\n    plt_cm.savefig(fig_name)\n    mlflow.log_artifact(fig_name, \"confusion-matrix-plot\")\n\n\n\nPipeline(steps=[('data_preprocessor',\n                 ColumnTransformer(transformers=[('encoder', OneHotEncoder(),\n                                                  ['BusinessTravel',\n                                                   'Department', 'Education',\n                                                   'EducationField',\n                                                   'EnvironmentSatisfaction',\n                                                   'Gender', 'JobInvolvement',\n                                                   'JobLevel',\n                                                   'JobSatisfaction',\n                                                   'MaritalStatus', 'OverTime',\n                                                   'PerformanceRating',\n                                                   'RelationshipSatisfaction',\n                                                   'StockOptionLevel',\n                                                   'TrainingTimesLastYe...\n       'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',\n       'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n       'YearsSinceLastPromotion', 'YearsWithCurrManager'],\n      dtype='object'))])),\n                ('classifier', RandomForestClassifier())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('data_preprocessor',\n                 ColumnTransformer(transformers=[('encoder', OneHotEncoder(),\n                                                  ['BusinessTravel',\n                                                   'Department', 'Education',\n                                                   'EducationField',\n                                                   'EnvironmentSatisfaction',\n                                                   'Gender', 'JobInvolvement',\n                                                   'JobLevel',\n                                                   'JobSatisfaction',\n                                                   'MaritalStatus', 'OverTime',\n                                                   'PerformanceRating',\n                                                   'RelationshipSatisfaction',\n                                                   'StockOptionLevel',\n                                                   'TrainingTimesLastYe...\n       'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',\n       'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n       'YearsSinceLastPromotion', 'YearsWithCurrManager'],\n      dtype='object'))])),\n                ('classifier', RandomForestClassifier())])data_preprocessor: ColumnTransformerColumnTransformer(transformers=[('encoder', OneHotEncoder(),\n                                 ['BusinessTravel', 'Department', 'Education',\n                                  'EducationField', 'EnvironmentSatisfaction',\n                                  'Gender', 'JobInvolvement', 'JobLevel',\n                                  'JobSatisfaction', 'MaritalStatus',\n                                  'OverTime', 'PerformanceRating',\n                                  'RelationshipSatisfaction',\n                                  'StockOptionLevel', 'TrainingTimesLastYear',\n                                  'WorkLifeBalance']),\n                                ('scaler', Sta...\n       'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel',\n       'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',\n       'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n       'YearsSinceLastPromotion', 'YearsWithCurrManager'],\n      dtype='object'))])encoder['BusinessTravel', 'Department', 'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MaritalStatus', 'OverTime', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TrainingTimesLastYear', 'WorkLifeBalance']OneHotEncoderOneHotEncoder()scalerIndex(['Age', 'DailyRate', 'DistanceFromHome', 'Education',\n       'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel',\n       'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',\n       'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n       'YearsSinceLastPromotion', 'YearsWithCurrManager'],\n      dtype='object')StandardScalerStandardScaler()RandomForestClassifierRandomForestClassifier()\n\n\n\n\naccuracy_score: 0.86\nprecision_score: 0.89\nrecall_score: 0.17\nf1_score: 0.29"
  },
  {
    "objectID": "04_mlflow_example.html#mlflow-models",
    "href": "04_mlflow_example.html#mlflow-models",
    "title": "MLflow in Practice",
    "section": "MLflow Models",
    "text": "MLflow Models\nMLflow logs a standard model format for each run and makes it available to be consumed by various downstream tools - for example, real-time serving through REST API or batch inference on Apache Spark.\n\nIn this tutorial, we will serve our model for real-time prediction using REST API.\n\nEach MLflow Model is a directory containing arbitrary files, together with an MLmodel file in the root of the directory that can define multiple flavors that the model can be viewed in. If we look at the structure of our logged scikit-learn model, it looks as follows:\nhr-attrition-model/\n├── MLmodel\n├── model.pkl\n├── conda.yaml\n├── python_env.yaml\n└── requirements.txt\nThe MLmodel file stores the information about all the flavours in which the saved model is available. In our case, the file describes two flavours.\nartifact_path: hr-attrition-model\nflavors:\n  python_function:\n    env: conda.yaml\n    loader_module: mlflow.sklearn\n    model_path: model.pkl\n    predict_fn: predict\n    python_version: 3.8.13\n  sklearn:\n    code: null\n    pickled_model: model.pkl\n    serialization_format: cloudpickle\n    sklearn_version: 1.1.3\nmlflow_version: 1.30.0\nThis model can then be used with any tool that supports either the sklearn or python_function model flavor. For example, the mlflow models serve command can serve a model with the python_function:\nmlflow models serve \\\n  -m 'runs:/cf70d23652c343e08a68cf9a88697528/hr-attrition-model' \\\n  --port 8123 \\\n  --no-conda\n\nMLflow Model Serving\nThe served MLflow model is available as an endpoint and can be used as follows:\n\nimport requests\n\napi_endpoint = \"http://localhost:8123/invocations\"\ndata = test_X[0:1].to_json(orient=\"records\")\nheaders = {\"Content-Type\": \"application/json\"}\nresponse = requests.post(url=api_endpoint, data=data, headers=headers)\nprediction = response.text\nprint(prediction)"
  }
]