[
  {
    "objectID": "01_ml_complexities.html#traditional-software-vs.-machine-learning",
    "href": "01_ml_complexities.html#traditional-software-vs.-machine-learning",
    "title": "Data & AI Engineering Academy",
    "section": "Traditional Software vs. Machine Learning",
    "text": "Developing machine learning applications is complex, and the complexity doesn’t necessarity strive from the fact that the theory behind the machine learning is difficult, or the math is difficult or the algorithm which are presented are difficult. Although there is some element of complexity, the machine learning frameworks makes it easier by abstracting away the details and exposing a simple class interface to initialise and train the model. In fact the default paramters gives pretty much a good baseline model to work with.\n\n\n\n\n\n\n\nTraditional Software\nMachine Learning\n\n\n\n\nGoal: Meet a functional specification\nGoal: Optimize metric (e.g. accuracy) Constantly experiment to improve it.\n\n\nQuality depends only on code\nQuality depends on input data and tuning parmeters\n\n\nTypically pick one software stack w/ fewer libraries and tools\nCompare + combine many libraries, models\n\n\nLimited deployment environments\nDiverse deployment environments"
  },
  {
    "objectID": "01_ml_complexities.html#machine-learning-lifecycle",
    "href": "01_ml_complexities.html#machine-learning-lifecycle",
    "title": "Data & AI Engineering Academy",
    "section": "Machine Learning Lifecycle",
    "text": "Following are four stages of a machine learning lifecycle, there can be other stages is between, but these are the paramount stages -\n\nData Ingestion - Depending on the nature of data you might use one of the many data technologies for staging the data e.g., Hadoop, Kafka, S3, Delta lake, etc.\nData Preparation - As per requirement and ease of use you might use a library in different language for preparing the data e.g., Spark, Pandas, Scikit-Learn, R, Java, etc.\nTraining - Depending on the problem and type of data you might use different libraries or framework for training the model e.g., Scikit-Learn, TensorFlow, PyTorch, Xgboost, etc.\nDeployment - Based on the type of model and how you want to serve the results there are many deployment options like Docker, Kubernetes, TensorFlow serving, Flask, etc."
  },
  {
    "objectID": "01_ml_complexities.html#challenges-in-managing-machine-learning-lifecycle",
    "href": "01_ml_complexities.html#challenges-in-managing-machine-learning-lifecycle",
    "title": "Data & AI Engineering Academy",
    "section": "Challenges in managing Machine Learning lifecycle",
    "text": "Each stage has it’s own requirements and tools\nSome stages (data preparation and training) rely on tuning parameters\nEach stage has it’s own scaling requirements\nEnsuring the same model that yielded optimal performance is deployed\nGovernance and provenance - how the model evolved, who used it, when it was used so on and so forth.\n\nTo solve these challenges, MLflow, an open source project, simplifies the entire ML lifecycle. MLflow introduces simple abstractions to package reproducible projects, track results, encapsulate models that can be used with many existing tools, and central repository to share models, accelerating the ML lifecycle for organizations of any size."
  },
  {
    "objectID": "02_mlflow_intro.html#mlflow-components",
    "href": "02_mlflow_intro.html#mlflow-components",
    "title": "Data & AI Engineering Academy",
    "section": "MLflow Components",
    "text": "MLflow currently offers four components:\n\n\n\n\n\n\n\nNote\n\n\n\nMLflow Models and MLflow Registry are not within the scope of this training.\n\n\n\nMLflow Tracking\nWhen you use MLflow model tracking, you can train a variety of different machine learning models then make predictions with them interchangeably using the standardized model prediction interface. You can also register your models in the MLflow model registry and keep track of which model is being used in production so that this information is easily accessible to everyone you are working with.\nMLflow Tracking is organized around the concept of runs, which are executions of some piece of data science code. Each run records the following information -\n\nParameters: Key-value inputs to your code\nMetrics: Numeric values (can update over time)\nTags and Notes: Additional information about a run\nArtifacts: Files, data, and models\nSource: Name of the file used to launch the run\nVersion: The version of the source code\nRun: An instance of code that runs by MLflow\nExperiment: {Run, …, Run}\nStart & End Time: Start and end time of a run\n\n\n\n\nRuns and Artifacts Store\nMLFlow provides wide variety of storage option for logging runs and artifacts.\n\nMLflow Runs\n\nThey can be recorded to local files, to a SQLAlchemy compatible database, or remotely to a tracking server.\nMLflow uses backend store component for storing runs\nBackend store persists MLflow entities (runs, parameters, metrics, tags, notes, metadata, etc.)\nBackend store options:\n\nA file store backend - local file path\nA database-backed store - mysql, mssql, sqlite, or postgresql\nHTTP server (specified as https://my-server:5000), which is a server hosting an MLflow tracking server.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, the MLflow Python API logs runs locally to files in an mlruns directory wherever you ran your program. You can then run mlflow ui to see the logged runs.\n\n\n\n\nMLflow Artifacts\n\nThey can be persisted to local files and a variety of remote file storage solutions.\nMLflow uses artifact store component for storing artifacts\nArtifact store persists artifacts (files, models, images, in-memory objects, or model summary, etc.)\nArtifact store options:\n\nLocal file path\nAmazon S3\nAzure Blob Storage\nGoogle Cloud Storage\nSFTP Server\nNFS"
  },
  {
    "objectID": "02_mlflow_intro.html#common-mlflow-configurations",
    "href": "02_mlflow_intro.html#common-mlflow-configurations",
    "title": "Data & AI Engineering Academy",
    "section": "Common MLflow configurations",
    "text": "Since the MLflow client can interface with a variety of backend and artifact storage configurations. We will look a three common scenarios:\n\n\n\nScenario 1 - MLflow on localhost\n\n\n\n\n\nScenario 2 - MLflow on localhost with backend store as an SQLAlchemy compatible database type: SQLite\n\n\n\n\n\nScenario 3 - Tracking server launched at localhost: mlflow server –backend-store-uri /workspace/mlruns"
  },
  {
    "objectID": "03_setup_mlflow.html#install",
    "href": "03_setup_mlflow.html#install",
    "title": "Data & AI Engineering Academy",
    "section": "Install",
    "text": "The easiest way to install MLflow is using pip as follows -\n\n\n\n\n\n\nNote\n\n\n\nMake sure you have created and activated the virtual environment using the python virtual environment manager of your choice.\n\n\n!pip install --upgrade pip\n!pip install --quiet mlflow\nMLflow comes with a rich CLI that provides a simple interface to various functionality in MLflow. You can use the CLI to run projects, start the tracking UI, create and list experiments, download run artifacts, serve MLflow Python Function and scikit-learn models, and serve models on Microsoft Azure Machine Learning and Amazon SageMaker.\n!mlflow --help\nIn the next section, we will use mlflow server <args> and mlflow ui commands to demonstrate different scenarios."
  },
  {
    "objectID": "03_setup_mlflow.html#scenario1",
    "href": "03_setup_mlflow.html#scenario1",
    "title": "Data & AI Engineering Academy",
    "section": "Scenario#1",
    "text": "MLflow on localhost\n\nThis is the most basic set up, where both backend and artifact store are set to local file store. It’s the default mode, so we don’t have to set any parameters while starting the tracking server.\nStart MLflow tracking server without any arguments.\nmlflow server"
  },
  {
    "objectID": "03_setup_mlflow.html#scenario2",
    "href": "03_setup_mlflow.html#scenario2",
    "title": "Data & AI Engineering Academy",
    "section": "Scenario#2",
    "text": "MLflow on localhost with backend store as an SQLAlchemy compatible database type: SQLite\n\nIn this case, artifacts are stored under a local directory, and MLflow entities are inserted in a SQLite database file mlruns.db.\nStart MLflow tracking server by specifying appropriate values for --backend-store-uri and --default-artifact-root\nmlflow server --backend-store-uri sqlite:////workspace/mlruns.db \\\n              --default-artifact-root /workspace/mlruns"
  },
  {
    "objectID": "03_setup_mlflow.html#scenario3",
    "href": "03_setup_mlflow.html#scenario3",
    "title": "Data & AI Engineering Academy",
    "section": "Scenario#3",
    "text": "Tracking server launched at localhost\n\nSimilar to scenario 1 but a tracking server is launched, listening for REST request calls at the default port 5000.\nStart MLflow tracking server by specifying local file path value for --backend-store-uri\nmlflow server --backend-store-uri /workspace/mlrun"
  },
  {
    "objectID": "03_setup_mlflow.html#mlflow-tracking-ui",
    "href": "03_setup_mlflow.html#mlflow-tracking-ui",
    "title": "Data & AI Engineering Academy",
    "section": "MLflow Tracking UI",
    "text": "After the scenario is set up, you can launch MLflow tracking UI by typing mlflow ui --port 6000 in a separate terminal.\nAt this point you would have MLflow tracking server running on port 5000 (http://localhost:5000) and MLflow Tracking UI running on port 6000. MLFlow Tracking UI communicates with the server to present results of each experiment from the set storage location. It also, allows to compare results across runs and experiments."
  },
  {
    "objectID": "04_mlflow.html#what-you-will-learn",
    "href": "04_mlflow.html#what-you-will-learn",
    "title": "Data & AI Engineering Academy",
    "section": "What you will learn",
    "text": "How each component of MLflow helps address challenges of the ML lifecycle.\n\nHow to use MLflow Tracking to record and query experiments: code, data, config, and results.\nHow to use MLflow Models general format to send models to diverse deployment tools.\nHow to use MLflow Tracking UI to visually compare and contrast experimental runs with different tuning parameters and evaluate metrics."
  }
]